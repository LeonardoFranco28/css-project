# CSS Scraper - Extractor de Planillas

## ğŸ“‹ DescripciÃ³n

Este proyecto permite extraer datos de empleados de la pÃ¡gina de transparencia de la Caja de Seguro Social de PanamÃ¡. El scraper ha sido refactorizado para incluir funcionalidades avanzadas como navegaciÃ³n automÃ¡tica por pÃ¡ginas y configuraciÃ³n de cantidad de registros.

## ğŸš€ CaracterÃ­sticas

### âœ¨ Nuevas Funcionalidades

1. **ConfiguraciÃ³n de registros por pÃ¡gina**: Cambia automÃ¡ticamente entre 10, 20 o 50 registros por pÃ¡gina
2. **NavegaciÃ³n automÃ¡tica**: Extrae datos de mÃºltiples pÃ¡ginas automÃ¡ticamente
3. **Scraping inteligente**: Detecta automÃ¡ticamente la estructura de la pÃ¡gina CSS
4. **MÃºltiples formatos de salida**: CSV y Parquet para mejor rendimiento
5. **Logs detallados**: Seguimiento completo del proceso de extracciÃ³n

### ğŸ“Š Datos ExtraÃ­dos

El scraper extrae la siguiente informaciÃ³n de cada empleado:

- IdentificaciÃ³n/PosiciÃ³n
- CÃ©dula
- Nombre completo
- Cargo
- Departamento
- Estatus (Permanente, etc.)
- Inicio en planilla
- Salario
- Gastos
- Sobre sueldo
- Total
- Objeto de gasto

## ğŸ›  InstalaciÃ³n

### Prerrequisitos

- Python 3.8+
- Google Chrome instalado
- ConexiÃ³n a internet

### Dependencias

```bash
pip install selenium polars webdriver-manager
```

## ğŸ“– Uso

### 1. Uso BÃ¡sico - Una pÃ¡gina

```python
from app.services.workers import extract

# Extraer datos de una sola pÃ¡gina (10 registros por defecto)
url = "https://transparencia.css.gob.pa/planilla/grid_defensoria/"
data = extract(url)
print(f"Registros extraÃ­dos: {data.shape[0]}")
```

### 2. Uso Avanzado - MÃºltiples pÃ¡ginas

```python
from app.services.workers import extract_all_pages

# Extraer datos de mÃºltiples pÃ¡ginas con 50 registros por pÃ¡gina
data = extract_all_pages(
    url="https://transparencia.css.gob.pa/planilla/grid_defensoria/",
    records_per_page=50,  # Opciones: 10, 20, 50
    max_pages=10         # NÃºmero mÃ¡ximo de pÃ¡ginas
)
print(f"Total de registros: {data.shape[0]}")
```

### 3. ConfiguraciÃ³n Personalizada

```python
from app.services.workers import scrape_with_config

# ConfiguraciÃ³n personalizada
config = {
    'records_per_page': 50,  # 10, 20 o 50
    'max_pages': 15,         # MÃ¡ximo de pÃ¡ginas
    'headless': True,        # Ejecutar sin interfaz
    'wait_time': 5          # Tiempo de espera en segundos
}

data = scrape_with_config(url, config)
```

### 4. Trabajar con Archivo Local

```python
from app.services.workers import extract_from_local_html

# Si tienes un archivo HTML descargado
data = extract_from_local_html("path/to/page.html")
```

### 5. Ejecutar Ejemplo Completo

```bash
python ejemplo_scraping.py
```

## ğŸ“ Estructura del Proyecto

```
css-project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ workers.py          # MÃ³dulo principal de scraping
â”‚   â”œâ”€â”€ db.py                   # Base de datos (si aplica)
â”‚   â””â”€â”€ main.py                 # AplicaciÃ³n principal
â”œâ”€â”€ ejemplo_scraping.py         # Ejemplo de uso
â”œâ”€â”€ page.html                   # PÃ¡gina HTML de muestra
â”œâ”€â”€ requirements.txt            # Dependencias
â””â”€â”€ README.md                   # Este archivo
```

## ğŸ”§ Funciones Principales

### `extract_all_pages()`

FunciÃ³n principal para scraping de mÃºltiples pÃ¡ginas.

**ParÃ¡metros:**
- `url`: URL del sitio web
- `records_per_page`: Cantidad de registros por pÃ¡gina (10, 20, 50)
- `max_pages`: NÃºmero mÃ¡ximo de pÃ¡ginas a extraer

### `scrape_with_config()`

Scraping con configuraciÃ³n personalizada.

**ParÃ¡metros:**
- `url`: URL del sitio web
- `config`: Diccionario con configuraciÃ³n personalizada

### `extract_from_local_html()`

Extrae datos de un archivo HTML local.

**ParÃ¡metros:**
- `file_path`: Ruta al archivo HTML local

## ğŸ“ Ejemplos de Salida

### Archivos Generados

El scraper genera automÃ¡ticamente:

- `employees_data.csv`: Datos en formato CSV
- `employees_data.parquet`: Datos en formato Parquet (mÃ¡s eficiente)

### EstadÃ­sticas de Ejemplo

```
=== INFORMACIÃ“N DEL DATASET ===
Filas: 250
Columnas: 12

Columnas disponibles: ['Identificacion / Posicion', 'CÃ©dula', 'Nombre completo', ...]

=== PRIMERAS 5 FILAS ===
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Identificacion /... â”‚ CÃ©dula    â”‚ Nombre completo â”‚ Cargo            â”‚ Departamento    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 10202000165         â”‚ 1-721-2263â”‚ JEROME CAITO    â”‚ ALMACENISTA I    â”‚ ADMINISTRACION  â”‚
â”‚ 10202000166         â”‚ 1-46-410  â”‚ LEONARDO ESP... â”‚ GUARDIAN         â”‚ ADMINISTRACION  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš ï¸ Consideraciones

### Rendimiento

- **50 registros por pÃ¡gina**: MÃ¡s eficiente para grandes volÃºmenes
- **Tiempo de espera**: Aumentar `wait_time` si la conexiÃ³n es lenta
- **Headless mode**: Usar `headless=True` para mejor rendimiento

### Buenas PrÃ¡cticas

1. **Respeta el sitio web**: No hagas demasiadas requests muy rÃ¡pido
2. **Maneja errores**: El scraper incluye manejo de errores robusto
3. **Guarda regularmente**: Los datos se guardan automÃ¡ticamente
4. **Monitorea logs**: Usa los logs para detectar problemas

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se encontraron filas de datos"

- Verificar que la URL sea correcta
- Aumentar el tiempo de espera
- Comprobar la conectividad a internet

### Error: "Timeout"

- Aumentar `wait_time` en la configuraciÃ³n
- Verificar la estabilidad de la conexiÃ³n
- Reintentar con menos pÃ¡ginas

### Chrome Driver Issues

El script usa `webdriver-manager` para manejar automÃ¡ticamente Chrome Driver. Si hay problemas:

```bash
pip install --upgrade webdriver-manager
```

## ğŸ“Š AnÃ¡lisis Posterior

### Con Pandas

```python
import pandas as pd

# Leer datos guardados
df = pd.read_csv('employees_data.csv')

# AnÃ¡lisis por departamento
dept_analysis = df.groupby('Departamento')['Salario'].agg(['count', 'mean', 'sum'])
print(dept_analysis)
```

### Con Polars (Recomendado)

```python
import polars as pl

# Leer datos (mÃ¡s eficiente)
df = pl.read_parquet('employees_data.parquet')

# AnÃ¡lisis rÃ¡pido
analysis = df.group_by('Departamento').agg([
    pl.count().alias('empleados'),
    pl.col('Salario').mean().alias('salario_promedio'),
    pl.col('Salario').sum().alias('total_salarios')
])
print(analysis)
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ”— Links Ãštiles

- [PÃ¡gina de Transparencia CSS](https://transparencia.css.gob.pa/planilla/grid_defensoria/)
- [DocumentaciÃ³n de Selenium](https://selenium-python.readthedocs.io/)
- [DocumentaciÃ³n de Polars](https://pola-rs.github.io/polars-book/)